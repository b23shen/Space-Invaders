{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.3.1 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (2.3.1)\n",
      "Requirement already satisfied: gym in c:\\programdata\\anaconda3\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: keras-rl2 in c:\\programdata\\anaconda3\\lib\\site-packages (1.0.4)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow==2.3.1) (1.18.4)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow==2.3.1) (2.3.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow==2.3.1) (1.1.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow==2.3.1) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow==2.3.1) (0.36.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow==2.3.1) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow==2.3.1) (2.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow==2.3.1) (1.12.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow==2.3.1) (1.15.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow==2.3.1) (1.6.3)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow==2.3.1) (1.34.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow==2.3.1) (3.14.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow==2.3.1) (2.10.0)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow==2.3.1) (0.3.3)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow==2.3.1) (0.11.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorflow==2.3.1) (3.3.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (1.2.1)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (1.2.1)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from gym) (6.1.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.7.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2.25.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (51.1.0.post20201221)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.3.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.4.2)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.24.0)\n",
      "Requirement already satisfied: future in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.26.2)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (2020.12.5)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (4.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.2.8)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.7.4.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\user\\appdata\\roaming\\python\\python37\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.1) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.3.1 gym keras-rl2 gym[atari]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Test Random Environment with OpenAI Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tetris', 'Adventure', 'AirRaid', 'Alien', 'Amidar', 'Assault', 'Asterix', 'Asteroids', 'Atlantis', 'Atlantis2', 'Backgammon', 'BankHeist', 'BasicMath', 'BattleZone', 'BeamRider', 'Berzerk', 'Blackjack', 'Bowling', 'Boxing', 'Breakout', 'Carnival', 'Casino', 'Centipede', 'ChopperCommand', 'CrazyClimber', 'Crossbow', 'Darkchambers', 'Defender', 'DemonAttack', 'DonkeyKong', 'DoubleDunk', 'Earthworld', 'ElevatorAction', 'Enduro', 'Entombed', 'Et', 'FishingDerby', 'FlagCapture', 'Freeway', 'Frogger', 'Frostbite', 'Galaxian', 'Gopher', 'Gravitar', 'Hangman', 'HauntedHouse', 'Hero', 'HumanCannonball', 'IceHockey', 'Jamesbond', 'JourneyEscape', 'Kaboom', 'Kangaroo', 'KeystoneKapers', 'KingKong', 'Klax', 'Koolaid', 'Krull', 'KungFuMaster', 'LaserGates', 'LostLuggage', 'MarioBros', 'MiniatureGolf', 'MontezumaRevenge', 'MrDo', 'MsPacman', 'NameThisGame', 'Othello', 'Pacman', 'Phoenix', 'Pitfall', 'Pitfall2', 'Pong', 'Pooyan', 'PrivateEye', 'Qbert', 'Riverraid', 'RoadRunner', 'Robotank', 'Seaquest', 'SirLancelot', 'Skiing', 'Solaris', 'SpaceInvaders', 'SpaceWar', 'StarGunner', 'Superman', 'Surround', 'Tennis', 'TicTacToe3D', 'TimePilot', 'Trondead', 'Turmoil', 'Tutankham', 'UpNDown', 'Venture', 'VideoCheckers', 'Videochess', 'Videocube', 'VideoPinball', 'WizardOfWor', 'WordZapper', 'YarsRevenge', 'Zaxxon']\n"
     ]
    }
   ],
   "source": [
    "# e.g., if you imported the supported version of Freeway\n",
    "#from ale_py.roms import Freeway\n",
    "\n",
    "# Print all registered ROMs\n",
    "import ale_py.roms as roms\n",
    "print(roms.__all__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import SpaceInvaders-v0 environment and get height, width, channels and actions\n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "height, width, channels = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We have 6 actions to choose\n",
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:355.0\n",
      "Episode:2 Score:35.0\n",
      "Episode:3 Score:105.0\n",
      "Episode:4 Score:105.0\n",
      "Episode:5 Score:515.0\n"
     ]
    }
   ],
   "source": [
    "# Test the games five times with random actions\n",
    "\n",
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = random.choice([0,1,2,3,4,5])\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create a Deep Learning Model with Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definite the model with cnn\n",
    "def build_model(height, width, channels, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, (8,8), strides=(4,4), activation='relu', input_shape=(3,height, width, channels)))\n",
    "    model.add(Convolution2D(64, (4,4), strides=(2,2), activation='relu'))\n",
    "    model.add(Convolution2D(64, (3,3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the model if necessary\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = build_model(height, width, channels, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 3, 51, 39, 32)     6176      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 3, 24, 18, 64)     32832     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 3, 22, 16, 64)     36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 67584)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               34603520  \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 34,812,326\n",
      "Trainable params: 34,812,326\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Show the summary of model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Build Agent with Keras-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Implement the epsilon greedy policy\\n\\n    Eps Greedy policy either:\\n\\n    - takes a random action with probability epsilon\\n    - takes current best action with prob (1 - epsilon)\\n    '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "\n",
    "# All agent is besed on DQN\n",
    "# An implementation of the DQN agent as described in Mnih (2013) and Mnih (2015).\n",
    "# http://arxiv.org/pdf/1312.5602.pdf\n",
    "# http://arxiv.org/abs/1509.06461\n",
    "\n",
    "\"\"\"Implement the linear annealing policy\n",
    "\n",
    "    Linear Annealing Policy computes a current threshold value and\n",
    "    transfers it to an inner policy which chooses the action. The threshold\n",
    "    value is following a linear function decreasing over time.\n",
    "    \"\"\"\n",
    "\n",
    "\"\"\"Implement the epsilon greedy policy\n",
    "\n",
    "    Eps Greedy policy either:\n",
    "\n",
    "    - takes a random action with probability epsilon\n",
    "    - takes current best action with prob (1 - epsilon)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent 1 EpsGreedyQPolicy eps = 0.05 nb_steps = 10000  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the agent 1\n",
    "\n",
    "def build_agent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05, nb_steps=10000)\n",
    "    memory = SequentialMemory(limit=1000, window_length=3)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  enable_dueling_network=True, dueling_type='avg', \n",
    "                   nb_actions=actions, nb_steps_warmup=1000\n",
    "                  )\n",
    "    return dqn\n",
    "\n",
    "\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  490/10000: episode: 1, duration: 8.483s, episode steps: 490, steps per second:  58, episode reward: 80.000, mean reward:  0.163 [ 0.000, 25.000], mean action: 2.359 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1462/10000: episode: 2, duration: 178.956s, episode steps: 972, steps per second:   5, episode reward: 240.000, mean reward:  0.247 [ 0.000, 30.000], mean action: 2.442 [0.000, 5.000],  loss: 7.163973, mean_q: 10.394617, mean_eps: 0.889210\n",
      " 2215/10000: episode: 3, duration: 286.084s, episode steps: 753, steps per second:   3, episode reward: 155.000, mean reward:  0.206 [ 0.000, 30.000], mean action: 2.471 [0.000, 5.000],  loss: 0.916519, mean_q: 9.545321, mean_eps: 0.834580\n",
      " 2994/10000: episode: 4, duration: 292.333s, episode steps: 779, steps per second:   3, episode reward: 240.000, mean reward:  0.308 [ 0.000, 30.000], mean action: 2.511 [0.000, 5.000],  loss: 1.143257, mean_q: 9.195589, mean_eps: 0.765640\n",
      " 3934/10000: episode: 5, duration: 352.635s, episode steps: 940, steps per second:   3, episode reward: 215.000, mean reward:  0.229 [ 0.000, 30.000], mean action: 2.662 [0.000, 5.000],  loss: 0.648432, mean_q: 9.225279, mean_eps: 0.688285\n",
      " 4730/10000: episode: 6, duration: 299.714s, episode steps: 796, steps per second:   3, episode reward: 210.000, mean reward:  0.264 [ 0.000, 30.000], mean action: 2.431 [0.000, 5.000],  loss: 0.428397, mean_q: 8.495248, mean_eps: 0.610165\n",
      " 5116/10000: episode: 7, duration: 145.435s, episode steps: 386, steps per second:   3, episode reward: 75.000, mean reward:  0.194 [ 0.000, 25.000], mean action: 2.557 [0.000, 5.000],  loss: 0.327842, mean_q: 8.793435, mean_eps: 0.556975\n",
      " 5561/10000: episode: 8, duration: 168.035s, episode steps: 445, steps per second:   3, episode reward: 105.000, mean reward:  0.236 [ 0.000, 30.000], mean action: 2.438 [0.000, 5.000],  loss: 0.368313, mean_q: 9.202681, mean_eps: 0.519580\n",
      " 5912/10000: episode: 9, duration: 132.140s, episode steps: 351, steps per second:   3, episode reward: 75.000, mean reward:  0.214 [ 0.000, 25.000], mean action: 2.425 [0.000, 5.000],  loss: 0.214591, mean_q: 9.765020, mean_eps: 0.483760\n",
      " 6304/10000: episode: 10, duration: 146.699s, episode steps: 392, steps per second:   3, episode reward: 55.000, mean reward:  0.140 [ 0.000, 20.000], mean action: 2.378 [0.000, 5.000],  loss: 0.298096, mean_q: 9.548392, mean_eps: 0.450325\n",
      " 7000/10000: episode: 11, duration: 259.467s, episode steps: 696, steps per second:   3, episode reward: 130.000, mean reward:  0.187 [ 0.000, 30.000], mean action: 2.358 [0.000, 5.000],  loss: 0.281604, mean_q: 8.963353, mean_eps: 0.401365\n",
      " 7684/10000: episode: 12, duration: 252.630s, episode steps: 684, steps per second:   3, episode reward: 70.000, mean reward:  0.102 [ 0.000, 20.000], mean action: 2.440 [0.000, 5.000],  loss: 0.183887, mean_q: 8.314646, mean_eps: 0.339265\n",
      " 8796/10000: episode: 13, duration: 411.017s, episode steps: 1112, steps per second:   3, episode reward: 470.000, mean reward:  0.423 [ 0.000, 200.000], mean action: 2.157 [0.000, 5.000],  loss: 3.747074, mean_q: 8.986318, mean_eps: 0.258445\n",
      " 9302/10000: episode: 14, duration: 185.491s, episode steps: 506, steps per second:   3, episode reward: 20.000, mean reward:  0.040 [ 0.000, 10.000], mean action: 1.883 [0.000, 5.000],  loss: 0.232924, mean_q: 8.466507, mean_eps: 0.185635\n",
      "done, took 3379.593 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x215198d7580>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use GPU to fasten the program\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Use DQN Agnet to fit the game environment\n",
    "dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "D:\\Anaconda\\lib\\site-packages\\gym\\envs\\atari\\environment.py:255: UserWarning: \u001b[33mWARN: We strongly suggest supplying `render_mode` when constructing your environment, e.g., gym.make(ID, render_mode='human'). Using `render_mode` provides access to proper scaling, audio support, and proper framerates.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: 190.000, steps: 887\n",
      "Episode 2: reward: 210.000, steps: 1411\n",
      "Episode 3: reward: 180.000, steps: 842\n",
      "Episode 4: reward: 75.000, steps: 587\n",
      "Episode 5: reward: 115.000, steps: 816\n",
      "Episode 6: reward: 80.000, steps: 658\n",
      "Episode 7: reward: 105.000, steps: 687\n",
      "Episode 8: reward: 185.000, steps: 1080\n",
      "Episode 9: reward: 320.000, steps: 833\n",
      "Episode 10: reward: 105.000, steps: 667\n",
      "156.5\n"
     ]
    }
   ],
   "source": [
    "# Test the performance of the agent\n",
    "scores = dqn.test(env, nb_episodes=10, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent 2 EpsGreedyQPolicy eps = 0.2 nb_steps = 10000  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the agent 2\n",
    "\n",
    "def build_agent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.2, nb_steps=10000)\n",
    "    memory = SequentialMemory(limit=1000, window_length=3)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  enable_dueling_network=True, dueling_type='avg', \n",
    "                   nb_actions=actions, nb_steps_warmup=1000\n",
    "                  )\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  600/10000: episode: 1, duration: 7.774s, episode steps: 600, steps per second:  77, episode reward:  5.000, mean reward:  0.008 [ 0.000,  5.000], mean action: 2.477 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1251/10000: episode: 2, duration: 100.120s, episode steps: 651, steps per second:   7, episode reward: 110.000, mean reward:  0.169 [ 0.000, 25.000], mean action: 2.521 [0.000, 5.000],  loss: 6.928415, mean_q: 3.872397, mean_eps: 0.898705\n",
      " 1756/10000: episode: 3, duration: 197.022s, episode steps: 505, steps per second:   3, episode reward: 80.000, mean reward:  0.158 [ 0.000, 25.000], mean action: 2.671 [0.000, 5.000],  loss: 0.579766, mean_q: 3.381719, mean_eps: 0.864730\n",
      " 2715/10000: episode: 4, duration: 363.882s, episode steps: 959, steps per second:   3, episode reward: 205.000, mean reward:  0.214 [ 0.000, 30.000], mean action: 2.462 [0.000, 5.000],  loss: 0.920727, mean_q: 4.207735, mean_eps: 0.798850\n",
      " 3246/10000: episode: 5, duration: 201.139s, episode steps: 531, steps per second:   3, episode reward: 105.000, mean reward:  0.198 [ 0.000, 25.000], mean action: 2.510 [0.000, 5.000],  loss: 0.322328, mean_q: 4.497552, mean_eps: 0.731800\n",
      " 4257/10000: episode: 6, duration: 380.532s, episode steps: 1011, steps per second:   3, episode reward: 365.000, mean reward:  0.361 [ 0.000, 30.000], mean action: 2.410 [0.000, 5.000],  loss: 0.659331, mean_q: 4.229386, mean_eps: 0.662410\n",
      " 4916/10000: episode: 7, duration: 245.856s, episode steps: 659, steps per second:   3, episode reward: 150.000, mean reward:  0.228 [ 0.000, 30.000], mean action: 2.601 [0.000, 5.000],  loss: 0.348953, mean_q: 4.281759, mean_eps: 0.587260\n",
      " 5765/10000: episode: 8, duration: 321.454s, episode steps: 849, steps per second:   3, episode reward: 255.000, mean reward:  0.300 [ 0.000, 30.000], mean action: 2.497 [0.000, 5.000],  loss: 0.418824, mean_q: 3.689626, mean_eps: 0.519400\n",
      " 6385/10000: episode: 9, duration: 234.960s, episode steps: 620, steps per second:   3, episode reward: 210.000, mean reward:  0.339 [ 0.000, 30.000], mean action: 2.369 [0.000, 5.000],  loss: 0.448256, mean_q: 4.317189, mean_eps: 0.453295\n",
      " 7344/10000: episode: 10, duration: 361.126s, episode steps: 959, steps per second:   3, episode reward: 290.000, mean reward:  0.302 [ 0.000, 30.000], mean action: 2.439 [0.000, 5.000],  loss: 0.411593, mean_q: 4.010284, mean_eps: 0.382240\n",
      " 7849/10000: episode: 11, duration: 190.133s, episode steps: 505, steps per second:   3, episode reward: 45.000, mean reward:  0.089 [ 0.000, 25.000], mean action: 2.329 [0.000, 5.000],  loss: 0.111341, mean_q: 2.706843, mean_eps: 0.316360\n",
      " 8530/10000: episode: 12, duration: 255.025s, episode steps: 681, steps per second:   3, episode reward: 110.000, mean reward:  0.162 [ 0.000, 30.000], mean action: 2.772 [0.000, 5.000],  loss: 0.176312, mean_q: 2.102088, mean_eps: 0.262990\n",
      " 9040/10000: episode: 13, duration: 191.253s, episode steps: 510, steps per second:   3, episode reward: 75.000, mean reward:  0.147 [ 0.000, 25.000], mean action: 2.745 [0.000, 5.000],  loss: 0.225126, mean_q: 2.889315, mean_eps: 0.209395\n",
      " 9712/10000: episode: 14, duration: 252.254s, episode steps: 672, steps per second:   3, episode reward: 80.000, mean reward:  0.119 [ 0.000, 20.000], mean action: 2.604 [0.000, 5.000],  loss: 0.090672, mean_q: 2.668546, mean_eps: 0.156205\n",
      "done, took 3410.712 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2039a4f3b80>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use GPU to fasten the program\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Use DQN Agnet to fit the game environment\n",
    "dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 255.000, steps: 932\n",
      "Episode 2: reward: 190.000, steps: 944\n",
      "Episode 3: reward: 495.000, steps: 1035\n",
      "Episode 4: reward: 35.000, steps: 521\n",
      "Episode 5: reward: 45.000, steps: 606\n",
      "Episode 6: reward: 280.000, steps: 1111\n",
      "Episode 7: reward: 110.000, steps: 593\n",
      "Episode 8: reward: 420.000, steps: 1604\n",
      "Episode 9: reward: 105.000, steps: 807\n",
      "Episode 10: reward: 50.000, steps: 372\n",
      "198.5\n"
     ]
    }
   ],
   "source": [
    "# Test the performance of the agent\n",
    "scores = dqn.test(env, nb_episodes=10, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent 3 EpsGreedyQPolicy eps = 0.2 nb_steps = 20000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the agent 3\n",
    "\n",
    "def build_agent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.2, nb_steps=20000)\n",
    "    memory = SequentialMemory(limit=1000, window_length=3)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  enable_dueling_network=True, dueling_type='avg', \n",
    "                   nb_actions=actions, nb_steps_warmup=1000\n",
    "                  )\n",
    "    return dqn\n",
    "\n",
    "\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   762/20000: episode: 1, duration: 10.924s, episode steps: 762, steps per second:  70, episode reward: 105.000, mean reward:  0.138 [ 0.000, 30.000], mean action: 2.524 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1678/20000: episode: 2, duration: 269.352s, episode steps: 916, steps per second:   3, episode reward: 285.000, mean reward:  0.311 [ 0.000, 30.000], mean action: 2.525 [0.000, 5.000],  loss: 7.871606, mean_q: 7.289139, mean_eps: 0.939745\n",
      "  2293/20000: episode: 3, duration: 240.216s, episode steps: 615, steps per second:   3, episode reward: 180.000, mean reward:  0.293 [ 0.000, 30.000], mean action: 2.561 [0.000, 5.000],  loss: 1.268222, mean_q: 8.170401, mean_eps: 0.910675\n",
      "  2722/20000: episode: 4, duration: 173.505s, episode steps: 429, steps per second:   2, episode reward: 75.000, mean reward:  0.175 [ 0.000, 25.000], mean action: 2.476 [0.000, 5.000],  loss: 1.276355, mean_q: 8.079276, mean_eps: 0.887185\n",
      "  4026/20000: episode: 5, duration: 512.924s, episode steps: 1304, steps per second:   3, episode reward: 650.000, mean reward:  0.498 [ 0.000, 200.000], mean action: 2.622 [0.000, 5.000],  loss: 4.177796, mean_q: 8.900995, mean_eps: 0.848193\n",
      "  4643/20000: episode: 6, duration: 248.829s, episode steps: 617, steps per second:   2, episode reward: 105.000, mean reward:  0.170 [ 0.000, 30.000], mean action: 2.624 [0.000, 5.000],  loss: 0.420715, mean_q: 7.431822, mean_eps: 0.804970\n",
      "  5210/20000: episode: 7, duration: 224.948s, episode steps: 567, steps per second:   3, episode reward: 15.000, mean reward:  0.026 [ 0.000, 10.000], mean action: 2.513 [0.000, 5.000],  loss: 0.137614, mean_q: 7.148491, mean_eps: 0.778330\n",
      "  6083/20000: episode: 8, duration: 342.425s, episode steps: 873, steps per second:   3, episode reward: 210.000, mean reward:  0.241 [ 0.000, 30.000], mean action: 2.546 [0.000, 5.000],  loss: 0.459987, mean_q: 6.947855, mean_eps: 0.745930\n",
      "  6637/20000: episode: 9, duration: 218.996s, episode steps: 554, steps per second:   3, episode reward: 80.000, mean reward:  0.144 [ 0.000, 25.000], mean action: 2.549 [0.000, 5.000],  loss: 0.223592, mean_q: 6.847542, mean_eps: 0.713823\n",
      "  7311/20000: episode: 10, duration: 262.142s, episode steps: 674, steps per second:   3, episode reward: 135.000, mean reward:  0.200 [ 0.000, 30.000], mean action: 2.657 [0.000, 5.000],  loss: 0.324907, mean_q: 7.294216, mean_eps: 0.686192\n",
      "  7723/20000: episode: 11, duration: 157.834s, episode steps: 412, steps per second:   3, episode reward: 80.000, mean reward:  0.194 [ 0.000, 20.000], mean action: 2.517 [0.000, 5.000],  loss: 0.273135, mean_q: 8.286262, mean_eps: 0.661757\n",
      "  8639/20000: episode: 12, duration: 360.840s, episode steps: 916, steps per second:   3, episode reward: 215.000, mean reward:  0.235 [ 0.000, 30.000], mean action: 2.368 [0.000, 5.000],  loss: 0.286692, mean_q: 8.179188, mean_eps: 0.631877\n",
      " 10094/20000: episode: 13, duration: 552.881s, episode steps: 1455, steps per second:   3, episode reward: 600.000, mean reward:  0.412 [ 0.000, 200.000], mean action: 2.434 [0.000, 5.000],  loss: 2.087406, mean_q: 7.476355, mean_eps: 0.578530\n",
      " 10684/20000: episode: 14, duration: 222.415s, episode steps: 590, steps per second:   3, episode reward: 115.000, mean reward:  0.195 [ 0.000, 20.000], mean action: 2.397 [0.000, 5.000],  loss: 1.259105, mean_q: 10.323420, mean_eps: 0.532517\n",
      " 11030/20000: episode: 15, duration: 130.216s, episode steps: 346, steps per second:   3, episode reward: 30.000, mean reward:  0.087 [ 0.000, 15.000], mean action: 2.448 [0.000, 5.000],  loss: 0.256178, mean_q: 10.266502, mean_eps: 0.511458\n",
      " 11526/20000: episode: 16, duration: 190.857s, episode steps: 496, steps per second:   3, episode reward: 40.000, mean reward:  0.081 [ 0.000, 10.000], mean action: 2.399 [0.000, 5.000],  loss: 0.190193, mean_q: 10.315839, mean_eps: 0.492512\n",
      " 12669/20000: episode: 17, duration: 437.046s, episode steps: 1143, steps per second:   3, episode reward: 430.000, mean reward:  0.376 [ 0.000, 200.000], mean action: 2.436 [0.000, 5.000],  loss: 2.524105, mean_q: 10.519375, mean_eps: 0.455635\n",
      " 13400/20000: episode: 18, duration: 278.090s, episode steps: 731, steps per second:   3, episode reward: 110.000, mean reward:  0.150 [ 0.000, 30.000], mean action: 2.639 [0.000, 5.000],  loss: 0.291921, mean_q: 9.255686, mean_eps: 0.413470\n",
      " 13802/20000: episode: 19, duration: 152.358s, episode steps: 402, steps per second:   3, episode reward: 90.000, mean reward:  0.224 [ 0.000, 25.000], mean action: 2.920 [0.000, 5.000],  loss: 0.388126, mean_q: 9.872365, mean_eps: 0.387977\n",
      " 14287/20000: episode: 20, duration: 184.519s, episode steps: 485, steps per second:   3, episode reward: 140.000, mean reward:  0.289 [ 0.000, 30.000], mean action: 2.507 [0.000, 5.000],  loss: 0.457306, mean_q: 11.282863, mean_eps: 0.368020\n",
      " 14943/20000: episode: 21, duration: 248.853s, episode steps: 656, steps per second:   3, episode reward: 80.000, mean reward:  0.122 [ 0.000, 20.000], mean action: 2.238 [0.000, 5.000],  loss: 0.230862, mean_q: 11.616054, mean_eps: 0.342347\n",
      " 15441/20000: episode: 22, duration: 187.510s, episode steps: 498, steps per second:   3, episode reward: 125.000, mean reward:  0.251 [ 0.000, 25.000], mean action: 2.104 [0.000, 5.000],  loss: 0.306782, mean_q: 10.109912, mean_eps: 0.316382\n",
      " 16265/20000: episode: 23, duration: 308.104s, episode steps: 824, steps per second:   3, episode reward: 235.000, mean reward:  0.285 [ 0.000, 30.000], mean action: 1.926 [0.000, 5.000],  loss: 0.370703, mean_q: 10.049812, mean_eps: 0.286637\n",
      " 16903/20000: episode: 24, duration: 238.729s, episode steps: 638, steps per second:   3, episode reward: 190.000, mean reward:  0.298 [ 0.000, 30.000], mean action: 1.983 [0.000, 5.000],  loss: 0.628000, mean_q: 9.523840, mean_eps: 0.253742\n",
      " 17600/20000: episode: 25, duration: 261.928s, episode steps: 697, steps per second:   3, episode reward: 225.000, mean reward:  0.323 [ 0.000, 30.000], mean action: 2.826 [0.000, 5.000],  loss: 0.514854, mean_q: 9.666994, mean_eps: 0.223705\n",
      " 18004/20000: episode: 26, duration: 150.568s, episode steps: 404, steps per second:   3, episode reward: 30.000, mean reward:  0.074 [ 0.000, 20.000], mean action: 2.460 [0.000, 5.000],  loss: 0.239231, mean_q: 9.923705, mean_eps: 0.198932\n",
      " 18591/20000: episode: 27, duration: 220.082s, episode steps: 587, steps per second:   3, episode reward: 80.000, mean reward:  0.136 [ 0.000, 20.000], mean action: 2.881 [0.000, 5.000],  loss: 0.191132, mean_q: 10.025317, mean_eps: 0.176635\n",
      " 19253/20000: episode: 28, duration: 248.989s, episode steps: 662, steps per second:   3, episode reward: 155.000, mean reward:  0.234 [ 0.000, 30.000], mean action: 2.582 [0.000, 5.000],  loss: 0.242382, mean_q: 9.453497, mean_eps: 0.148532\n",
      "done, took 7315.649 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a91aba07c0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use GPU to fasten the program\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Use DQN Agnet to fit the game environment\n",
    "dqn.fit(env, nb_steps=20000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 345.000, steps: 1393\n",
      "Episode 2: reward: 35.000, steps: 564\n",
      "Episode 3: reward: 125.000, steps: 763\n",
      "Episode 4: reward: 200.000, steps: 810\n",
      "Episode 5: reward: 230.000, steps: 843\n",
      "Episode 6: reward: 120.000, steps: 599\n",
      "Episode 7: reward: 400.000, steps: 835\n",
      "Episode 8: reward: 140.000, steps: 625\n",
      "Episode 9: reward: 105.000, steps: 539\n",
      "Episode 10: reward: 110.000, steps: 668\n",
      "181.0\n"
     ]
    }
   ],
   "source": [
    "# Test the performance of the agent\n",
    "scores = dqn.test(env, nb_episodes=10, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of Agent 3 is better than that of Agent 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent 4 EpsGreedyQPolicy eps = 0.05 nb_steps = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Build the agent 4\n",
    "\n",
    "def build_agent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05, nb_steps=20000)\n",
    "    memory = SequentialMemory(limit=1000, window_length=3)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  enable_dueling_network=True, dueling_type='avg', \n",
    "                   nb_actions=actions, nb_steps_warmup=1000\n",
    "                  )\n",
    "    return dqn\n",
    "\n",
    "\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   739/20000: episode: 1, duration: 6.028s, episode steps: 739, steps per second: 123, episode reward: 135.000, mean reward:  0.183 [ 0.000, 30.000], mean action: 2.356 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1540/20000: episode: 2, duration: 204.768s, episode steps: 801, steps per second:   4, episode reward: 175.000, mean reward:  0.218 [ 0.000, 30.000], mean action: 2.325 [0.000, 5.000],  loss: 1.551605, mean_q: 5.847624, mean_eps: 0.942850\n",
      "  2150/20000: episode: 3, duration: 229.309s, episode steps: 610, steps per second:   3, episode reward: 30.000, mean reward:  0.049 [ 0.000, 10.000], mean action: 2.605 [0.000, 5.000],  loss: 0.171828, mean_q: 5.474777, mean_eps: 0.916998\n",
      "  2698/20000: episode: 4, duration: 207.425s, episode steps: 548, steps per second:   3, episode reward: 65.000, mean reward:  0.119 [ 0.000, 20.000], mean action: 2.520 [0.000, 5.000],  loss: 0.158851, mean_q: 5.277823, mean_eps: 0.890942\n",
      "  3538/20000: episode: 5, duration: 317.719s, episode steps: 840, steps per second:   3, episode reward: 125.000, mean reward:  0.149 [ 0.000, 25.000], mean action: 2.536 [0.000, 5.000],  loss: 0.242780, mean_q: 5.453539, mean_eps: 0.859712\n",
      "  4597/20000: episode: 6, duration: 404.054s, episode steps: 1059, steps per second:   3, episode reward: 350.000, mean reward:  0.331 [ 0.000, 30.000], mean action: 2.315 [0.000, 5.000],  loss: 0.368527, mean_q: 5.483057, mean_eps: 0.816985\n",
      "  5374/20000: episode: 7, duration: 294.920s, episode steps: 777, steps per second:   3, episode reward: 185.000, mean reward:  0.238 [ 0.000, 30.000], mean action: 2.224 [0.000, 5.000],  loss: 0.363468, mean_q: 6.413800, mean_eps: 0.775675\n",
      "  5982/20000: episode: 8, duration: 228.878s, episode steps: 608, steps per second:   3, episode reward: 80.000, mean reward:  0.132 [ 0.000, 25.000], mean action: 2.516 [0.000, 5.000],  loss: 0.225255, mean_q: 5.939507, mean_eps: 0.744512\n",
      "  7490/20000: episode: 9, duration: 567.531s, episode steps: 1508, steps per second:   3, episode reward: 215.000, mean reward:  0.143 [ 0.000, 30.000], mean action: 2.719 [0.000, 5.000],  loss: 0.294324, mean_q: 6.472011, mean_eps: 0.696902\n",
      "  9012/20000: episode: 10, duration: 574.029s, episode steps: 1522, steps per second:   3, episode reward: 265.000, mean reward:  0.174 [ 0.000, 30.000], mean action: 2.514 [0.000, 5.000],  loss: 0.268431, mean_q: 6.170831, mean_eps: 0.628727\n",
      "  9922/20000: episode: 11, duration: 341.598s, episode steps: 910, steps per second:   3, episode reward: 155.000, mean reward:  0.170 [ 0.000, 30.000], mean action: 2.148 [0.000, 5.000],  loss: 0.226992, mean_q: 5.582924, mean_eps: 0.574008\n",
      " 10432/20000: episode: 12, duration: 191.529s, episode steps: 510, steps per second:   3, episode reward: 80.000, mean reward:  0.157 [ 0.000, 20.000], mean action: 2.610 [0.000, 5.000],  loss: 0.496487, mean_q: 5.931933, mean_eps: 0.542057\n",
      " 11305/20000: episode: 13, duration: 327.170s, episode steps: 873, steps per second:   3, episode reward: 200.000, mean reward:  0.229 [ 0.000, 30.000], mean action: 3.126 [0.000, 5.000],  loss: 0.346442, mean_q: 5.738924, mean_eps: 0.510940\n",
      " 12088/20000: episode: 14, duration: 292.398s, episode steps: 783, steps per second:   3, episode reward: 180.000, mean reward:  0.230 [ 0.000, 30.000], mean action: 2.683 [0.000, 5.000],  loss: 0.349631, mean_q: 6.552189, mean_eps: 0.473680\n",
      " 12484/20000: episode: 15, duration: 148.108s, episode steps: 396, steps per second:   3, episode reward: 80.000, mean reward:  0.202 [ 0.000, 20.000], mean action: 1.992 [0.000, 5.000],  loss: 0.386855, mean_q: 6.712611, mean_eps: 0.447152\n",
      " 12877/20000: episode: 16, duration: 147.114s, episode steps: 393, steps per second:   3, episode reward: 75.000, mean reward:  0.191 [ 0.000, 25.000], mean action: 2.662 [0.000, 5.000],  loss: 0.205698, mean_q: 6.610056, mean_eps: 0.429400\n",
      " 13962/20000: episode: 17, duration: 404.753s, episode steps: 1085, steps per second:   3, episode reward: 345.000, mean reward:  0.318 [ 0.000, 30.000], mean action: 3.048 [0.000, 5.000],  loss: 0.464770, mean_q: 6.542359, mean_eps: 0.396145\n",
      " 14736/20000: episode: 18, duration: 289.489s, episode steps: 774, steps per second:   3, episode reward: 180.000, mean reward:  0.233 [ 0.000, 30.000], mean action: 2.894 [0.000, 5.000],  loss: 0.365548, mean_q: 6.921602, mean_eps: 0.354317\n",
      " 15705/20000: episode: 19, duration: 362.067s, episode steps: 969, steps per second:   3, episode reward: 260.000, mean reward:  0.268 [ 0.000, 30.000], mean action: 2.423 [0.000, 5.000],  loss: 0.403653, mean_q: 6.990267, mean_eps: 0.315100\n",
      " 16395/20000: episode: 20, duration: 257.388s, episode steps: 690, steps per second:   3, episode reward: 75.000, mean reward:  0.109 [ 0.000, 30.000], mean action: 2.139 [0.000, 5.000],  loss: 0.215329, mean_q: 6.502825, mean_eps: 0.277772\n",
      " 16976/20000: episode: 21, duration: 216.937s, episode steps: 581, steps per second:   3, episode reward: 105.000, mean reward:  0.181 [ 0.000, 30.000], mean action: 2.281 [0.000, 5.000],  loss: 0.393009, mean_q: 6.110666, mean_eps: 0.249175\n",
      " 17661/20000: episode: 22, duration: 256.118s, episode steps: 685, steps per second:   3, episode reward: 165.000, mean reward:  0.241 [ 0.000, 30.000], mean action: 2.264 [0.000, 5.000],  loss: 0.208146, mean_q: 5.928202, mean_eps: 0.220690\n",
      " 18504/20000: episode: 23, duration: 314.254s, episode steps: 843, steps per second:   3, episode reward: 410.000, mean reward:  0.486 [ 0.000, 200.000], mean action: 3.231 [0.000, 5.000],  loss: 2.849758, mean_q: 6.909998, mean_eps: 0.186310\n",
      " 18866/20000: episode: 24, duration: 132.613s, episode steps: 362, steps per second:   3, episode reward: 50.000, mean reward:  0.138 [ 0.000, 20.000], mean action: 2.602 [0.000, 5.000],  loss: 0.292152, mean_q: 6.994837, mean_eps: 0.159197\n",
      " 19428/20000: episode: 25, duration: 206.013s, episode steps: 562, steps per second:   3, episode reward: 110.000, mean reward:  0.196 [ 0.000, 30.000], mean action: 3.007 [0.000, 5.000],  loss: 0.554796, mean_q: 5.657150, mean_eps: 0.138407\n",
      "done, took 7131.790 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a91ab80e80>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use GPU to fasten the program\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Use DQN Agnet to fit the game environment\n",
    "dqn.fit(env, nb_steps=20000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\gym\\envs\\atari\\environment.py:255: UserWarning: \u001b[33mWARN: We strongly suggest supplying `render_mode` when constructing your environment, e.g., gym.make(ID, render_mode='human'). Using `render_mode` provides access to proper scaling, audio support, and proper framerates.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: 335.000, steps: 913\n",
      "Episode 2: reward: 70.000, steps: 416\n",
      "Episode 3: reward: 10.000, steps: 453\n",
      "Episode 4: reward: 215.000, steps: 874\n",
      "Episode 5: reward: 110.000, steps: 638\n",
      "Episode 6: reward: 250.000, steps: 984\n",
      "Episode 7: reward: 40.000, steps: 326\n",
      "Episode 8: reward: 380.000, steps: 993\n",
      "Episode 9: reward: 110.000, steps: 682\n",
      "Episode 10: reward: 135.000, steps: 711\n",
      "165.5\n"
     ]
    }
   ],
   "source": [
    "# Test the performance of the agent\n",
    "scores = dqn.test(env, nb_episodes=10, visualize=True)\n",
    "print(np.mean(scores.history['episode_reward']))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of Agent 2 is worse than that of Agent 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent 5 BoltzmannQPolicy nb_steps = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implement the Boltzmann Q Policy\n",
    "\n",
    "    Boltzmann Q Policy builds a probability law on q values and returns\n",
    "    an action selected randomly according to this law.\n",
    "    \"\"\"\n",
    "\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "\n",
    "\n",
    "# Build the agent 5\n",
    "\n",
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=1000, window_length=3)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  enable_dueling_network=True, dueling_type='avg', \n",
    "                   nb_actions=actions, nb_steps_warmup=1000\n",
    "                  )\n",
    "    return dqn\n",
    "\n",
    "\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1091/10000: episode: 1, duration: 42.917s, episode steps: 1091, steps per second:  25, episode reward: 225.000, mean reward:  0.206 [ 0.000, 30.000], mean action: 3.057 [0.000, 5.000],  loss: 3.858079, mean_q: 4.985610\n",
      " 1792/10000: episode: 2, duration: 263.096s, episode steps: 701, steps per second:   3, episode reward: 105.000, mean reward:  0.150 [ 0.000, 30.000], mean action: 2.919 [0.000, 5.000],  loss: 0.863608, mean_q: 5.090210\n",
      " 2591/10000: episode: 3, duration: 299.888s, episode steps: 799, steps per second:   3, episode reward: 215.000, mean reward:  0.269 [ 0.000, 30.000], mean action: 2.852 [0.000, 5.000],  loss: 0.484350, mean_q: 4.841578\n",
      " 3350/10000: episode: 4, duration: 284.071s, episode steps: 759, steps per second:   3, episode reward: 155.000, mean reward:  0.204 [ 0.000, 30.000], mean action: 2.702 [0.000, 5.000],  loss: 0.404858, mean_q: 5.305809\n",
      " 4169/10000: episode: 5, duration: 306.690s, episode steps: 819, steps per second:   3, episode reward: 220.000, mean reward:  0.269 [ 0.000, 30.000], mean action: 2.325 [0.000, 5.000],  loss: 0.397247, mean_q: 4.952459\n",
      " 4601/10000: episode: 6, duration: 161.441s, episode steps: 432, steps per second:   3, episode reward: 75.000, mean reward:  0.174 [ 0.000, 25.000], mean action: 2.361 [0.000, 5.000],  loss: 0.310952, mean_q: 4.384581\n",
      " 5145/10000: episode: 7, duration: 203.277s, episode steps: 544, steps per second:   3, episode reward: 105.000, mean reward:  0.193 [ 0.000, 25.000], mean action: 2.465 [0.000, 5.000],  loss: 0.261310, mean_q: 3.954508\n",
      " 5646/10000: episode: 8, duration: 187.423s, episode steps: 501, steps per second:   3, episode reward: 85.000, mean reward:  0.170 [ 0.000, 30.000], mean action: 2.250 [0.000, 5.000],  loss: 0.328969, mean_q: 3.894587\n",
      " 6706/10000: episode: 9, duration: 396.230s, episode steps: 1060, steps per second:   3, episode reward: 335.000, mean reward:  0.316 [ 0.000, 30.000], mean action: 2.448 [0.000, 5.000],  loss: 0.504797, mean_q: 4.070856\n",
      " 7743/10000: episode: 10, duration: 387.538s, episode steps: 1037, steps per second:   3, episode reward: 200.000, mean reward:  0.193 [ 0.000, 30.000], mean action: 2.621 [0.000, 5.000],  loss: 0.475667, mean_q: 4.238256\n",
      " 8422/10000: episode: 11, duration: 254.486s, episode steps: 679, steps per second:   3, episode reward: 65.000, mean reward:  0.096 [ 0.000, 20.000], mean action: 2.389 [0.000, 5.000],  loss: 0.351971, mean_q: 4.147311\n",
      " 8950/10000: episode: 12, duration: 198.579s, episode steps: 528, steps per second:   3, episode reward: 125.000, mean reward:  0.237 [ 0.000, 25.000], mean action: 2.506 [0.000, 5.000],  loss: 0.338362, mean_q: 3.967373\n",
      " 9408/10000: episode: 13, duration: 171.812s, episode steps: 458, steps per second:   3, episode reward: 80.000, mean reward:  0.175 [ 0.000, 20.000], mean action: 2.476 [0.000, 5.000],  loss: 0.297450, mean_q: 3.970788\n",
      "done, took 3379.188 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aa4ffe6700>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use GPU to fasten the program\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Use DQN Agnet to fit the game environment\n",
    "dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: 120.000, steps: 816\n",
      "Episode 2: reward: 155.000, steps: 876\n",
      "Episode 3: reward: 50.000, steps: 648\n",
      "Episode 4: reward: 180.000, steps: 863\n",
      "Episode 5: reward: 245.000, steps: 1117\n",
      "Episode 6: reward: 245.000, steps: 1894\n",
      "Episode 7: reward: 240.000, steps: 1940\n",
      "Episode 8: reward: 135.000, steps: 838\n",
      "Episode 9: reward: 210.000, steps: 866\n",
      "Episode 10: reward: 180.000, steps: 892\n",
      "176.0\n"
     ]
    }
   ],
   "source": [
    "# Test the performance of the agent\n",
    "scores = dqn.test(env, nb_episodes=10, visualize=True)\n",
    "print(np.mean(scores.history['episode_reward']))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent 5 chose the most left side to fire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent 6 MaxBoltzmannQPolicy nb_steps = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    " \"\"\"\n",
    "    A combination of the eps-greedy and Boltzman q-policy.\n",
    "\n",
    "    Wiering, M.: Explorations in Efficient Reinforcement Learning.\n",
    "    PhD thesis, University of Amsterdam, Amsterdam (1999)\n",
    "\n",
    "    https://pure.uva.nl/ws/files/3153478/8461_UBA003000033.pdf\n",
    "    \"\"\"\n",
    "from rl.policy import MaxBoltzmannQPolicy\n",
    "\n",
    "# Build the agent 6\n",
    "\n",
    "def build_agent(model, actions):\n",
    "    policy = MaxBoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=1000, window_length=3)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  enable_dueling_network=True, dueling_type='avg', \n",
    "                   nb_actions=actions, nb_steps_warmup=1000\n",
    "                  )\n",
    "    return dqn\n",
    "\n",
    "\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "  544/10000: episode: 1, duration: 4.411s, episode steps: 544, steps per second: 123, episode reward: 110.000, mean reward:  0.202 [ 0.000, 30.000], mean action: 2.528 [0.000, 5.000],  loss: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1486/10000: episode: 2, duration: 185.578s, episode steps: 942, steps per second:   5, episode reward: 160.000, mean reward:  0.170 [ 0.000, 30.000], mean action: 2.807 [0.000, 5.000],  loss: 1.214559, mean_q: -0.595408\n",
      " 2385/10000: episode: 3, duration: 338.062s, episode steps: 899, steps per second:   3, episode reward: 135.000, mean reward:  0.150 [ 0.000, 30.000], mean action: 2.880 [0.000, 5.000],  loss: 1.011598, mean_q: -0.650642\n",
      " 2986/10000: episode: 4, duration: 225.965s, episode steps: 601, steps per second:   3, episode reward: 140.000, mean reward:  0.233 [ 0.000, 30.000], mean action: 3.559 [0.000, 5.000],  loss: 1.846490, mean_q: -0.370972\n",
      " 4040/10000: episode: 5, duration: 395.554s, episode steps: 1054, steps per second:   3, episode reward: 540.000, mean reward:  0.512 [ 0.000, 200.000], mean action: 3.202 [0.000, 5.000],  loss: 4.038543, mean_q: -0.242950\n",
      " 5325/10000: episode: 6, duration: 482.070s, episode steps: 1285, steps per second:   3, episode reward: 370.000, mean reward:  0.288 [ 0.000, 200.000], mean action: 2.607 [0.000, 5.000],  loss: 13.843902, mean_q: 0.030100\n",
      " 5836/10000: episode: 7, duration: 191.635s, episode steps: 511, steps per second:   3, episode reward: 60.000, mean reward:  0.117 [ 0.000, 20.000], mean action: 2.239 [0.000, 5.000],  loss: 13.762792, mean_q: -0.277165\n",
      " 6799/10000: episode: 8, duration: 360.659s, episode steps: 963, steps per second:   3, episode reward: 280.000, mean reward:  0.291 [ 0.000, 30.000], mean action: 1.273 [0.000, 5.000],  loss: 1.144863, mean_q: -0.240562\n",
      " 7754/10000: episode: 9, duration: 358.861s, episode steps: 955, steps per second:   3, episode reward: 150.000, mean reward:  0.157 [ 0.000, 25.000], mean action: 0.842 [0.000, 5.000],  loss: 1.620443, mean_q: -0.300350\n",
      " 8438/10000: episode: 10, duration: 261.303s, episode steps: 684, steps per second:   3, episode reward: 85.000, mean reward:  0.124 [ 0.000, 20.000], mean action: 1.010 [0.000, 5.000],  loss: 0.930679, mean_q: -0.285384\n",
      " 8808/10000: episode: 11, duration: 140.392s, episode steps: 370, steps per second:   3, episode reward: 40.000, mean reward:  0.108 [ 0.000, 25.000], mean action: 2.173 [0.000, 5.000],  loss: 0.925995, mean_q: -0.315883\n",
      " 9438/10000: episode: 12, duration: 238.241s, episode steps: 630, steps per second:   3, episode reward: 10.000, mean reward:  0.016 [ 0.000,  5.000], mean action: 0.743 [0.000, 5.000],  loss: 0.506411, mean_q: -0.451174\n",
      "done, took 3394.352 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aa674b89a0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use GPU to fasten the program\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Use DQN Agnet to fit the game environment\n",
    "dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 510.000, steps: 1620\n",
      "Episode 2: reward: 575.000, steps: 1244\n",
      "Episode 3: reward: 235.000, steps: 676\n",
      "Episode 4: reward: 205.000, steps: 559\n",
      "Episode 5: reward: 305.000, steps: 744\n",
      "Episode 6: reward: 435.000, steps: 1409\n",
      "Episode 7: reward: 300.000, steps: 1119\n",
      "Episode 8: reward: 485.000, steps: 1239\n",
      "Episode 9: reward: 215.000, steps: 742\n",
      "Episode 10: reward: 230.000, steps: 1162\n",
      "349.5\n"
     ]
    }
   ],
   "source": [
    "# Test the performance of the agent\n",
    "scores = dqn.test(env, nb_episodes=10, visualize=True)\n",
    "print(np.mean(scores.history['episode_reward']))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent 6 chose the most right side to fire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent 7 GreedyQPolicy nb_steps = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Implement the greedy policy\n",
    "\n",
    "    Greedy policy returns the current best action according to q_values\n",
    "    \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "    A combination of the eps-greedy and Boltzman q-policy.\n",
    "\n",
    "    Wiering, M.: Explorations in Efficient Reinforcement Learning.\n",
    "    PhD thesis, University of Amsterdam, Amsterdam (1999)\n",
    "\n",
    "    https://pure.uva.nl/ws/files/3153478/8461_UBA003000033.pdf\n",
    "    \"\"\"\n",
    "from rl.policy import GreedyQPolicy\n",
    "\n",
    "# Build the agent 7\n",
    "\n",
    "def build_agent(model, actions):\n",
    "    policy = GreedyQPolicy()\n",
    "    memory = SequentialMemory(limit=1000, window_length=3)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  enable_dueling_network=True, dueling_type='avg', \n",
    "                   nb_actions=actions, nb_steps_warmup=1000\n",
    "                  )\n",
    "    return dqn\n",
    "\n",
    "\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      " 1338/10000: episode: 1, duration: 138.801s, episode steps: 1338, steps per second:  10, episode reward: 365.000, mean reward:  0.273 [ 0.000, 30.000], mean action: 1.134 [1.000, 4.000],  loss: 3.657223, mean_q: 5.468041\n",
      " 1894/10000: episode: 2, duration: 208.865s, episode steps: 556, steps per second:   3, episode reward: 105.000, mean reward:  0.189 [ 0.000, 30.000], mean action: 1.092 [1.000, 4.000],  loss: 1.062975, mean_q: 5.216114\n",
      " 2572/10000: episode: 3, duration: 251.443s, episode steps: 678, steps per second:   3, episode reward: 160.000, mean reward:  0.236 [ 0.000, 30.000], mean action: 1.376 [1.000, 4.000],  loss: 0.835172, mean_q: 5.812083\n",
      " 2973/10000: episode: 4, duration: 149.811s, episode steps: 401, steps per second:   3, episode reward: 65.000, mean reward:  0.162 [ 0.000, 20.000], mean action: 1.352 [1.000, 4.000],  loss: 0.645570, mean_q: 5.529462\n",
      " 3314/10000: episode: 5, duration: 126.153s, episode steps: 341, steps per second:   3, episode reward: 55.000, mean reward:  0.161 [ 0.000, 25.000], mean action: 1.273 [1.000, 4.000],  loss: 0.682342, mean_q: 5.289823\n",
      " 3827/10000: episode: 6, duration: 189.168s, episode steps: 513, steps per second:   3, episode reward: 105.000, mean reward:  0.205 [ 0.000, 30.000], mean action: 1.345 [1.000, 4.000],  loss: 1.177012, mean_q: 5.051224\n",
      " 4731/10000: episode: 7, duration: 341.455s, episode steps: 904, steps per second:   3, episode reward: 230.000, mean reward:  0.254 [ 0.000, 30.000], mean action: 1.501 [1.000, 4.000],  loss: 1.123031, mean_q: 5.635777\n",
      " 5384/10000: episode: 8, duration: 253.754s, episode steps: 653, steps per second:   3, episode reward: 130.000, mean reward:  0.199 [ 0.000, 30.000], mean action: 1.680 [1.000, 4.000],  loss: 0.932600, mean_q: 5.962289\n",
      " 6092/10000: episode: 9, duration: 265.886s, episode steps: 708, steps per second:   3, episode reward: 270.000, mean reward:  0.381 [ 0.000, 30.000], mean action: 3.962 [1.000, 4.000],  loss: 1.234658, mean_q: 6.173247\n",
      " 6808/10000: episode: 10, duration: 268.842s, episode steps: 716, steps per second:   3, episode reward: 270.000, mean reward:  0.377 [ 0.000, 30.000], mean action: 4.001 [4.000, 5.000],  loss: 2.246937, mean_q: 5.635709\n",
      " 7537/10000: episode: 11, duration: 272.515s, episode steps: 729, steps per second:   3, episode reward: 270.000, mean reward:  0.370 [ 0.000, 30.000], mean action: 3.988 [1.000, 4.000],  loss: 2.279038, mean_q: 5.553091\n",
      " 8264/10000: episode: 12, duration: 269.546s, episode steps: 727, steps per second:   3, episode reward: 270.000, mean reward:  0.371 [ 0.000, 30.000], mean action: 3.977 [1.000, 5.000],  loss: 1.623663, mean_q: 5.640329\n",
      " 9765/10000: episode: 13, duration: 554.377s, episode steps: 1501, steps per second:   3, episode reward: 800.000, mean reward:  0.533 [ 0.000, 200.000], mean action: 3.994 [1.000, 4.000],  loss: 5.877421, mean_q: 5.366750\n",
      "done, took 3378.681 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28a2e8944c0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use GPU to fasten the program\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Use DQN Agnet to fit the game environment\n",
    "dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\gym\\envs\\atari\\environment.py:255: UserWarning: \u001b[33mWARN: We strongly suggest supplying `render_mode` when constructing your environment, e.g., gym.make(ID, render_mode='human'). Using `render_mode` provides access to proper scaling, audio support, and proper framerates.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: 270.000, steps: 727\n",
      "Episode 2: reward: 270.000, steps: 718\n",
      "Episode 3: reward: 270.000, steps: 722\n",
      "Episode 4: reward: 270.000, steps: 730\n",
      "Episode 5: reward: 270.000, steps: 711\n",
      "Episode 6: reward: 270.000, steps: 724\n",
      "Episode 7: reward: 265.000, steps: 692\n",
      "Episode 8: reward: 270.000, steps: 727\n",
      "Episode 9: reward: 270.000, steps: 722\n",
      "Episode 10: reward: 270.000, steps: 713\n",
      "269.5\n"
     ]
    }
   ],
   "source": [
    "# Test the performance of the agent\n",
    "scores = dqn.test(env, nb_episodes=10, visualize=True)\n",
    "print(np.mean(scores.history['episode_reward']))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent 7 chose the most right side to fire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent 8 BoltzmannGumbelQPolicy nb_steps = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Implements Boltzmann-Gumbel exploration (BGE) adapted for Q learning\n",
    "    based on the paper Boltzmann Exploration Done Right\n",
    "    (https://arxiv.org/pdf/1705.10257.pdf).\n",
    "\n",
    "    BGE is invariant with respect to the mean of the rewards but not their\n",
    "    variance. The parameter C, which defaults to 1, can be used to correct for\n",
    "    this, and should be set to the least upper bound on the standard deviation\n",
    "    of the rewards.\n",
    "\n",
    "    BGE is only available for training, not testing. For testing purposes, you\n",
    "    can achieve approximately the same result as BGE after training for N steps\n",
    "    on K actions with parameter C by using the BoltzmannQPolicy and setting\n",
    "    tau = C/sqrt(N/K).\"\"\"\n",
    "\n",
    "from rl.policy import BoltzmannGumbelQPolicy\n",
    "\n",
    "# Build the agent 8\n",
    "\n",
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannGumbelQPolicy()\n",
    "    memory = SequentialMemory(limit=1000, window_length=3)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  enable_dueling_network=True, dueling_type='avg', \n",
    "                   nb_actions=actions, nb_steps_warmup=1000\n",
    "                  )\n",
    "    return dqn\n",
    "\n",
    "\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "  464/10000: episode: 1, duration: 3.770s, episode steps: 464, steps per second: 123, episode reward: 85.000, mean reward:  0.183 [ 0.000, 25.000], mean action: 2.403 [0.000, 5.000],  loss: --, mean_q: --\n",
      " 1776/10000: episode: 2, duration: 295.968s, episode steps: 1312, steps per second:   4, episode reward: 335.000, mean reward:  0.255 [ 0.000, 200.000], mean action: 2.218 [0.000, 5.000],  loss: 27.783009, mean_q: 0.757882\n",
      " 2145/10000: episode: 3, duration: 136.340s, episode steps: 369, steps per second:   3, episode reward: 45.000, mean reward:  0.122 [ 0.000, 15.000], mean action: 2.672 [0.000, 5.000],  loss: 3.589642, mean_q: 0.515953\n",
      " 2804/10000: episode: 4, duration: 242.086s, episode steps: 659, steps per second:   3, episode reward: 105.000, mean reward:  0.159 [ 0.000, 30.000], mean action: 3.209 [0.000, 5.000],  loss: 0.500140, mean_q: 0.363015\n",
      " 3477/10000: episode: 5, duration: 246.467s, episode steps: 673, steps per second:   3, episode reward: 105.000, mean reward:  0.156 [ 0.000, 30.000], mean action: 4.012 [0.000, 5.000],  loss: 1.537087, mean_q: 0.381173\n",
      " 4356/10000: episode: 6, duration: 322.211s, episode steps: 879, steps per second:   3, episode reward: 210.000, mean reward:  0.239 [ 0.000, 30.000], mean action: 4.849 [0.000, 5.000],  loss: 2.334941, mean_q: 0.487323\n",
      " 5319/10000: episode: 7, duration: 353.482s, episode steps: 963, steps per second:   3, episode reward: 285.000, mean reward:  0.296 [ 0.000, 30.000], mean action: 4.884 [0.000, 5.000],  loss: 2.560661, mean_q: 0.445429\n",
      " 6279/10000: episode: 8, duration: 351.187s, episode steps: 960, steps per second:   3, episode reward: 285.000, mean reward:  0.297 [ 0.000, 30.000], mean action: 4.932 [0.000, 5.000],  loss: 3.063551, mean_q: 0.506225\n",
      " 7249/10000: episode: 9, duration: 355.885s, episode steps: 970, steps per second:   3, episode reward: 285.000, mean reward:  0.294 [ 0.000, 30.000], mean action: 4.912 [0.000, 5.000],  loss: 2.928290, mean_q: 0.491401\n",
      " 8212/10000: episode: 10, duration: 351.884s, episode steps: 963, steps per second:   3, episode reward: 285.000, mean reward:  0.296 [ 0.000, 30.000], mean action: 4.926 [0.000, 5.000],  loss: 3.290909, mean_q: 0.523740\n",
      " 9186/10000: episode: 11, duration: 357.780s, episode steps: 974, steps per second:   3, episode reward: 285.000, mean reward:  0.293 [ 0.000, 30.000], mean action: 4.957 [0.000, 5.000],  loss: 2.887471, mean_q: 0.479581\n",
      "done, took 3320.273 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bb185f7a90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use GPU to fasten the program\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Use DQN Agnet to fit the game environment\n",
    "dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\gym\\envs\\atari\\environment.py:255: UserWarning: \u001b[33mWARN: We strongly suggest supplying `render_mode` when constructing your environment, e.g., gym.make(ID, render_mode='human'). Using `render_mode` provides access to proper scaling, audio support, and proper framerates.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: 285.000, steps: 986\n",
      "Episode 2: reward: 285.000, steps: 971\n",
      "Episode 3: reward: 285.000, steps: 969\n",
      "Episode 4: reward: 285.000, steps: 974\n",
      "Episode 5: reward: 285.000, steps: 960\n",
      "Episode 6: reward: 285.000, steps: 963\n",
      "Episode 7: reward: 285.000, steps: 967\n",
      "Episode 8: reward: 285.000, steps: 970\n",
      "Episode 9: reward: 285.000, steps: 978\n",
      "Episode 10: reward: 285.000, steps: 982\n",
      "285.0\n"
     ]
    }
   ],
   "source": [
    "# Test the performance of the agent\n",
    "scores = dqn.test(env, nb_episodes=10, visualize=True)\n",
    "print(np.mean(scores.history['episode_reward']))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent 8 chose the most left side to fire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent 10 EpsGreedyQPolicy eps=0.1 nb_steps = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.policy import EpsGreedyQPolicy\n",
    "\n",
    "# Build the agent 10\n",
    "\n",
    "def build_agent(model, actions):\n",
    "    policy = EpsGreedyQPolicy()\n",
    "    memory = SequentialMemory(limit=1000, window_length=3)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  enable_dueling_network=True, dueling_type='avg', \n",
    "                   nb_actions=actions, nb_steps_warmup=1000\n",
    "                  )\n",
    "    return dqn\n",
    "\n",
    "\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "  698/10000: episode: 1, duration: 5.808s, episode steps: 698, steps per second: 120, episode reward: 340.000, mean reward:  0.487 [ 0.000, 30.000], mean action: 3.881 [0.000, 5.000],  loss: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py:2470: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1962/10000: episode: 2, duration: 366.105s, episode steps: 1264, steps per second:   3, episode reward: 210.000, mean reward:  0.166 [ 0.000, 30.000], mean action: 3.882 [0.000, 5.000],  loss: 2.973462, mean_q: 1.114301\n",
      " 2539/10000: episode: 3, duration: 215.324s, episode steps: 577, steps per second:   3, episode reward: 255.000, mean reward:  0.442 [ 0.000, 30.000], mean action: 3.821 [0.000, 5.000],  loss: 1.689626, mean_q: 0.988069\n",
      " 3736/10000: episode: 4, duration: 447.433s, episode steps: 1197, steps per second:   3, episode reward: 425.000, mean reward:  0.355 [ 0.000, 200.000], mean action: 1.850 [0.000, 5.000],  loss: 11.793288, mean_q: 1.301746\n",
      " 4506/10000: episode: 5, duration: 287.909s, episode steps: 770, steps per second:   3, episode reward: 180.000, mean reward:  0.234 [ 0.000, 30.000], mean action: 1.162 [0.000, 5.000],  loss: 17.854282, mean_q: 1.182583\n",
      " 6153/10000: episode: 6, duration: 614.431s, episode steps: 1647, steps per second:   3, episode reward: 550.000, mean reward:  0.334 [ 0.000, 200.000], mean action: 3.154 [0.000, 5.000],  loss: 7.177501, mean_q: 1.124135\n",
      " 6769/10000: episode: 7, duration: 241.637s, episode steps: 616, steps per second:   3, episode reward: 220.000, mean reward:  0.357 [ 0.000, 30.000], mean action: 3.826 [0.000, 5.000],  loss: 25.116760, mean_q: 1.351631\n",
      " 7876/10000: episode: 8, duration: 420.903s, episode steps: 1107, steps per second:   3, episode reward: 235.000, mean reward:  0.212 [ 0.000, 30.000], mean action: 2.801 [0.000, 5.000],  loss: 2.597513, mean_q: 1.114649\n",
      " 8558/10000: episode: 9, duration: 257.047s, episode steps: 682, steps per second:   3, episode reward: 135.000, mean reward:  0.198 [ 0.000, 30.000], mean action: 3.035 [0.000, 5.000],  loss: 2.422619, mean_q: 1.087471\n",
      " 9197/10000: episode: 10, duration: 242.179s, episode steps: 639, steps per second:   3, episode reward: 120.000, mean reward:  0.188 [ 0.000, 25.000], mean action: 3.103 [0.000, 5.000],  loss: 1.573990, mean_q: 1.024209\n",
      "done, took 3398.756 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28918a5e940>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use GPU to fasten the program\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Use DQN Agnet to fit the game environment\n",
    "dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\gym\\envs\\atari\\environment.py:255: UserWarning: \u001b[33mWARN: We strongly suggest supplying `render_mode` when constructing your environment, e.g., gym.make(ID, render_mode='human'). Using `render_mode` provides access to proper scaling, audio support, and proper framerates.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: 270.000, steps: 728\n",
      "Episode 2: reward: 270.000, steps: 706\n",
      "Episode 3: reward: 270.000, steps: 729\n",
      "Episode 4: reward: 270.000, steps: 730\n",
      "Episode 5: reward: 270.000, steps: 719\n",
      "Episode 6: reward: 270.000, steps: 719\n",
      "Episode 7: reward: 270.000, steps: 714\n",
      "Episode 8: reward: 270.000, steps: 713\n",
      "Episode 9: reward: 270.000, steps: 717\n",
      "Episode 10: reward: 270.000, steps: 712\n",
      "270.0\n"
     ]
    }
   ],
   "source": [
    "# Test the performance of the agent\n",
    "scores = dqn.test(env, nb_episodes=10, visualize=True)\n",
    "print(np.mean(scores.history['episode_reward']))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Saving and Reloading Agent from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('SavedWeights/10k-Fast/dqn_weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('SavedWeights/10k-Fast/dqn_weights2.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('SavedWeights/10k-Fast/dqn_weights3.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('SavedWeights/10k-Fast/dqn_weights4.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('SavedWeights/10k-Fast/dqn_weights5.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('SavedWeights/10k-Fast/dqn_weights6.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('SavedWeights/10k-Fast/dqn_weights7.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] SavedWeights/10k-Fast/dqn_weights8.h5f.index already exists - overwrite? [y/n]y\n",
      "[TIP] Next time specify overwrite=True!\n"
     ]
    }
   ],
   "source": [
    "dqn.save_weights('SavedWeights/10k-Fast/dqn_weights8.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('SavedWeights/10k-Fast/dqn_weights10.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('SavedWeights/10k-Fast/dqn_weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('SavedWeights/10k-Fast/dqn_weights2.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('SavedWeights/10k-Fast/dqn_weights3.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('SavedWeights/10k-Fast/dqn_weights4.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('SavedWeights/10k-Fast/dqn_weights5.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('SavedWeights/10k-Fast/dqn_weights6.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('SavedWeights/10k-Fast/dqn_weights7.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('SavedWeights/10k-Fast/dqn_weights8.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('SavedWeights/10k-Fast/dqn_weights10.h5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For EpsGreedyQPolicy, if the eps is low, the increasing of nb_steps will cause the performance become worse, while the eps is high, the increasing of nb_steps will cause the perfomance become better.\n",
    "\n",
    "For most policies, the agent will choose the most left or right side to fire and those which choose the right side can get higher socre may because the plane can shoot extra planes on the process from the most left side to the most right side.\n",
    "\n",
    "MaxBoltzmannQPolicy is the best policy to choose and it can get 312 on average and the best score is 645."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
